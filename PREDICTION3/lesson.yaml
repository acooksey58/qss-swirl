- Class: meta
  Course: qss-swirl
  Lesson: PREDICTION3
  Author: Elisha Cohen, Kosuke Imai
  Type: Standard
  Organization: Princeton University
  Version: 2.2.21

- Class: text
  Output: 'The following exercises are a companion to Chapter 4: Prediction.'

## Conceptual Questions
# Question 1
- Class: mult_question
  Output: 'How can regression models be used to draw causal inference?'
  AnswerChoices: >
    by predicting counterfactual outcomes;
    by randomizing outcomes;
    by fitting a line
  CorrectAnswer: by predicting counterfactual outcomes
  AnswerTests: omnitest(correctVal='by predicting counterfactual outcomes')
  Hint: See 'Regression and Causation'

# Question 2
- Class: mult_question
  Output: 'Association ________________ imply causation.'
  AnswerChoices: 'will always; will never; does not necessarily'
  CorrectAnswer: does not necessarily
  AnswerTests: omnitest(correctVal='does not necessarily')
  Hint: See 'Regression and Causation'

# Question 3
- Class: mult_question
  Output: >
    The main reason it is difficult to draw causal inference in
    observational studies is due to __________.
  AnswerChoices: 'confounders; associations; errors'
  CorrectAnswer: confounders
  AnswerTests: omnitest(correctVal='confounders')
  Hint: See 'Randomized Experiments'

# Question 4
- Class: mult_question
  Output: >
    A regression discontinuity design attempts to make causal inferences by
    analyzing observations lying __________ a given threshold.
  AnswerChoices: 'closely across; far away from'
  CorrectAnswer: closely across
  AnswerTests: omnitest(correctVal='closely across')
  Hint: See 'Regression Discontinuity design'

# Question 5
- Class: mult_question
  Output: >
    Even if the average treatment effect is positive,
    the same treatment may not affect everyone in the positive direction.
    This is an example of _________.
  AnswerChoices: >
    heterogenous treatment effects; homogenous treatment effects; both of these
  CorrectAnswer: heterogenous treatment effects
  AnswerTests: omnitest(correctVal='heterogenous treatment effects')
  Hint: See 'Heterogenous treatment effects'

## Programming Questions
- Class: cmd_question
  Output: >
    A dataset called "df" has been pre-loaded for you.
    It contains three variables x1, x2 and y. Regress y on x1 and x2.
  CorrectAnswer: lm(y ~ x1 + x2, data = df)
  AnswerTests: >-
    any_of_exprs(
      'lm(y ~ x1 + x2, data = df)',
      'lm(df$y ~ df$x1 + df$x2, data = df)',
      'lm(df$y ~ df$x1 + df$x2)'
    )
  Hint: See 'Regression with Multiple Predictors'

# Question 6
- Class: cmd_question
  Output: >
    Regress y on x1 and x2 again, this time omitting the intercept
    and saving it to the object "fit".
  CorrectAnswer: fit <- lm(y ~ -1 + x1 + x2, data = df)
  AnswerTests: >-
    any_of_exprs(
      'fit <- lm(y ~ -1 + x1 + x2, data = df)',
      'fit <- lm(df$y ~ -1 + df$x1 + df$x2, data = df)',
      'fit <- lm(df$y ~ -1 + df$x1 + df$x2)'
    )
  Hint: See 'Regression with Multiple Predictors'

# Question 7
- Class: cmd_question
  Output: 'Use an R function to call the coefficients of "fit".'
  CorrectAnswer: coef(fit)
  AnswerTests: >-
    any_of_exprs(
      'coef(fit)',
      'fit$coef',
      'fit$coefficients',
      'coefficients(fit)'
    )
  Hint: See 'Regression with Multiple Predictors'

# Question 8
- Class: cmd_question
  Output: 'Calculate the difference in coefficients between x2 and x1 from fit.'
  CorrectAnswer: coef(fit)["x2"] - coef(fit)["x1"]
  AnswerTests: >-
    any_of_exprs(
      'coef(fit)["x2"] - coef(fit)["x1"]',
      'coef(fit)["x1"] - coef(fit)["x2"]',
      'coef(fit)[2] - coef(fit)[1]',
      'coef(fit)[1] - coef(fit)[2]',
      'coefficients(fit)["x2"] - coefficients(fit)["x1"]',
      'coefficients(fit)["x1"] - coefficients(fit)["x2"]',
      'coefficients(fit)[2] - coefficients(fit)[1]',
      'coefficients(fit)[1] - coefficients(fit)[2]',
      'fit$coef["x2"] - fit$coef["x1"]',
      'fit$coef["x1"] - fit$coef["x2"]',
      'fit$coef[2] - fit$coef[1]',
      'fit$coef[1] - fit$coef[2]',
      'fit$coefficients["x2"] - fit$coefficients["x1"]',
      'fit$coefficients["x1"] - fit$coefficients["x2"]',
      'fit$coefficients[2] - fit$coefficients[1]',
      'fit$coefficients[1] - fit$coefficients[2]'
    )
  Hint: See 'Regression with Multiple Predictors'

# Question 9
- Class: cmd_question
  Output: >
    Run the regression again this time including x1 squared as regressor.
    You should also include an intercept and x1 and x2.
    Save it to an object called "fit2".
    Make sure to use the I() function when squaring the x1 term.
  CorrectAnswer: fit2 <- lm(y ~ x1 + I(x1^2) + x2, data = df)
  AnswerTests: >-
    any_of_exprs(
      'fit2 <- lm(y ~ x1 + I(x1^2) + x2, data = df)',
      'fit2 <- lm(y ~ x2 + I(x1^2) + x1, data = df)',
      'fit2 <- lm(y ~ x1 + x2 + I(x1^2), data = df)',
      'fit2 <- lm(y ~ x2 + x1 + I(x1^2), data = df)',
      'fit2 <- lm(y ~ I(x1^2) + x1 + x2, data = df)',
      'fit2 <- lm(y ~ I(x1^2) + x2 + x1, data = df)',
      'fit2 <- lm(df$y ~ df$x1 + I(df$x1^2) + df$x2)',
      'fit2 <- lm(df$y ~ df$x1 + df$x2 + I(df$x1^2))',
      'fit2 <- lm(df$y ~ df$x2 + I(df$x1^2) + df$x1)',
      'fit2 <- lm(df$y ~ df$x2 + df$x1 + I(df$x1^2))',
      'fit2 <- lm(df$y ~ I(df$x1^2) + df$x2 + df$x1)',
      'fit2 <- lm(df$y ~ I(df$x1^2) + df$x1 + df$x2)'
    )
  Hint: See 'Regression with Multiple Predictors'

# Question 10
- Class: cmd_question
  Output: 'Use a function to predict the fitted values of "fit2".'
  CorrectAnswer: predict(fit2)
  AnswerTests: >-
    any_of_exprs(
      'predict(fit2)',
      'fitted(fit2)',
      'fit2$fitted.values'
    )
  Hint: See 'Regression with Multiple Predictors'

- Class: text
  Output: >
    You've successfully completed part 3 of the Prediction course!

