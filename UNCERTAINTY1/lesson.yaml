- Class: meta
  Course: qss-swirl
  Lesson: UNCERTAINTY1
  Author: Evan Chow, Kosuke Imai
  Type: Standard
  Organization: Princeton University
  Version: 1.1.1

# Question 1
- Class: mult_question
  Output: >
    Which statement best describes how complete randomization differs
    from simple randomization?
  AnswerChoices: >
    Only part of our sample is randomly chosen;
    We randomize part of the sample but do not randomize the treatment;
    We choose how much of the sample receives treatment a priori;
    We randomize the sample, a priori
  CorrectAnswer: 'We choose how much of the sample receives treatment a priori'
  AnswerTests: >-
    omnitest(
      correctVal =
        'We choose how much of the sample receives treatment a priori'
    )
  Hint: See 7.1.1.

# Question 2
- Class: mult_question
  Output: >
    Let variance and bias be denoted by V(x) and B(x) respectively.
    Then Mean Squared Error (MSE) equals:
  AnswerChoices: 'V(x^2) + B(x); V(x) + B(x^2); V(x)^2 + B(x); V(x) + B(x)^2'
  CorrectAnswer: 'V(x) + B(x)^2'
  AnswerTests: omnitest(correctVal='V(x) + B(x)^2')
  Hint: See 7.1.2.

# Question 3
- Class: cmd_question
  Output: >
    The variance in a population is 5.
    For a sample of size 10 from that population, what is the variance in the
    sample mean (express your answer to the nearest 0.1)?
  CorrectAnswer: '0.5'
  AnswerTests: omnitest(correctExpr='0.5')
  Hint: See 7.1.2. Use the Central Limit Theorem.

# Question 4
- Class: mult_question
  Output: 'In the context of confidence intervals, what is alpha?'
  AnswerChoices: >
    The probability that over repeated sampling the confidence interval does not
    contain the true value of a parameter;
    The probability that the confidence interval contains the true value of a
    parameter, based on sample size;
    The probability that the confidence interval contains the true value of a
    parameter, regardless of sample size;
    The bias in the probability that the confidence interval contains the true
    value of a parameter'
  CorrectAnswer: >
    The probability that over repeated sampling the confidence interval does not
    contain the true value of a parameter
  AnswerTests: >-
    omnitest(
      correctVal =
        'The probability that over repeated sampling the confidence interval does not contain the true value of a parameter'
    )
  Hint: See 7.1.2.

# Question 5
- Class: cmd_question
  Output: >
    In our sample of voters we find that 70% of the participants support Obama.
    If we want our 95% confidence interval for the true population proportion
    to be within +/- 1% of the true value, what is the minimum number of
    participants we must ask?
  CorrectAnswer: '8068'
  AnswerTests: omnitest(correctExpr='8068')
  Hint: >
    See 7.1.3. How does margin of error allow us to determine the sample size
    given a predetermined level of significance?

# Question 6
- Class:  mult_question
  Output: >
    Retrospective bias corrected confidence intervals differ from the
    prospective bias corrected confidence intervals because:
  AnswerChoices: >
    Actually, both correction methods do not differ at all;
    In the prospective method the magnitude of the bias is estimated without
    observing the true parameter values; In practice the retrospective bias
    corrected confidence intervals have a lower coverage rate;
    In the prospective method more data points are needed in its estimation
  CorrectAnswer: >
    In the prospective method the magnitude of the bias is estimated without
    observing the true parameter values
  AnswerTests: >-
    omnitest(
      correctVal =
        'In the prospective method the magnitude of the bias is estimated without observing the true parameter values'
    )
  Hint: See 7.1.3.

# Question 7
- Class: mult_question
  Output: >
    How many degrees of freedom does a t-statistic require for a sample of
    size n?
  AnswerChoices: 'n; n-1; n-2; sqrt(n)'
  AnswerTests: omnitest(correctVal='n-1')
  Hint: See 7.1.4.

# Question 8
- Class: cmd_question
  Output: >
    I flip a fair coin 27 times. What is the variance in the expected number of
    heads (express to the nearest 0.001)?
  CorrectAnswer: '0.009'
  AnswerTests: omnitest(correctExpr='0.009')
  Hint: See 7.1.2. What does the Central Limit Theorem say?

# Question 9
- Class: cmd_question
  Output: >
    Now, I flip a coin of unknown bias 10 times.
    It comes up heads 8 times. What is the (non-negative) margin of error,
    calculated from a 95% confidence interval for the true probability the
    coin comes up heads (to the nearest 0.01)?
  CorrectAnswer: '0.25'
  AnswerTests: omnitest(correctExpr='0.25')
  Hint: See 7.1.3.

# Question 10
- Class: mult_question
  Output: >
    In a sample 50% of the times a coin has landed tails.
    For 95% confidence and a margin of error of 0.001, roughly how many flips
    should you we need to check if the coin is actually fair?
  AnswerChoices: '100000; 500000; 1000000; 5000000'
  AnswerTests: omnitest(correctVal='1000000')
  Hint: See 7.1.3. What does this say about sample size?

# Question 11
- Class: mult_question
  Output: >
    The variance of a Bernoulli distributed random variable is given by: p(1-p).
    What is the value of p that maximizes the variance of such a variable?
  AnswerChoices: '0; 1/2; 1/3; 1/4'
  AnswerTests: omnitest(correctVal='1/2')
  Hint: See 7.1.3.

- Class: text
  Output: >
    You've successfully completed part 1 of the Uncertainty course!
